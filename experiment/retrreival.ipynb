{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44a38b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4e6c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = \"../chroma_db_experiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f250d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_id = \"mxbai-embed-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc307937",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OllamaEmbeddings(model = embedding_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae177041",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\engma\\AppData\\Local\\Temp\\ipykernel_22940\\3479936724.py:1: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vector_store = Chroma(\n"
     ]
    }
   ],
   "source": [
    "vector_store = Chroma(\n",
    "    persist_directory = persist_directory,\n",
    "    embedding_function=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f98ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\engma\\AppData\\Local\\Temp\\ipykernel_22940\\4020431038.py:23: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm_gamma = Ollama(model=model_id_gemma, base_url=llm_url, temperature=0.2)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "system_prompt = \"\"\"\n",
    "You are the lawyer and author of the contract agreement written in the organisation Indian Oil Corporation Limited.\n",
    "Your job is to help the contractors and the officers of the organisation to do theiur job better by making them understand various clause agreements,\n",
    "quote the necessary contract agreements if asked about the related contract agreements.\n",
    "Only extract answers which are relevant to the question.\n",
    "If no context is available, use the question as reference.\n",
    "Context : {context}\n",
    "\"\"\"\n",
    "\n",
    "retriever = vector_store.as_retriever(search_kwargs = {'k' : 1})\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system_prompt),\n",
    "                                           (\"human\", \"{input}\")])\n",
    "\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "model_id_gemma = \"gemma2:2b\"\n",
    "llm_url = \"http://localhost:11434\"\n",
    "\n",
    "llm_gamma = Ollama(model=model_id_gemma, base_url=llm_url, temperature=0.2)\n",
    "\n",
    "compressor = LLMChainExtractor.from_llm(llm_gamma)\n",
    "\n",
    "compressor_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor = compressor, base_retriever = retriever\n",
    ")\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "question_answer_chain_gamma = create_stuff_documents_chain(llm_gamma, prompt)\n",
    "chain_gamma = create_retrieval_chain(compressor_retriever, question_answer_chain_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7680a7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How payment shall be done corresponding to measurement of the work done?\"\n",
    "\n",
    "response_sample_gamma = chain_gamma.invoke({\"input\" : question})\n",
    "print(\"Gamma response: \", response_sample_gamma[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d4458c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
